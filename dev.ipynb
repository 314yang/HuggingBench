{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff218e1e-25f9-4f98-a5e3-c223bb26e069",
   "metadata": {},
   "source": [
    "# 工作\n",
    "\n",
    "对 Hugging Face 构建知识图谱：\n",
    "- 构建 AI 场景的要素，涉及模型（model ）、数据集（dataset ）、空间（space ，可理解为模型部署 / 展示环境 ）、集合（collection ，数据或模型的集合归类 ）、用户（user ）、组织（organization ）、任务（task ）、论文（paper ），是整个流程的 “输入” 。\n",
    "- 梳理模型、数据、用户、机构等之间的复杂关联，让各要素关系更清晰：\n",
    "    - 模型与数据：像 BERT（经典预训练模型 ）在 Wikipedia 数据集上训练（trained on ），medBERT 在 BookCorpus、PubMed 等数据上训练，体现模型训练的数据依赖；Embedding Models（嵌入模型 ）由 Google 等发布（publish ）、定义（define for ），BERT 会被论文引用（cite ） 。\n",
    "    - 用户与模型：用户（如 Bob、Jack ）会有 “like（喜好 ）”“follow（关注 ）” 模型的行为，模型也可被用户微调（finetune ），体现人与模型的交互 。\n",
    "知识图谱的应用功能，辅助资源利用、任务处理和模型管理。\n",
    "- Resource Recommendation（资源推荐）：依据用户 “like（喜好 ）” 等行为，从数据库等资源里，给用户推荐模型、数据集等资源 。\n",
    "- Task Classification（任务分类 ）：基于数据、模型，为任务（如具体 NLP 任务 ）做定义、分类，匹配合适工具。\n",
    "- Model Tracing（模型溯源）：通过 “finetune（微调 ）” 等行为，追踪模型的迭代、衍生关系（比如一个模型微调后产生新模型，可追溯 lineage ）。\n",
    "\n",
    "<img src='https://img.shields.io/badge/arXiv-2505.17507-b31b1b.svg'>\n",
    "\n",
    "\n",
    "# 文件目录结构\n",
    "\n",
    "```plaintext\n",
    "HuggingBench/\n",
    "├── LICENSE                   # 项目使用的 Apache 2.0 许可证文件\n",
    "├── README.md                 # 项目的主说明文件，包含项目概述、数据链接、实验步骤等信息\n",
    "├── figs/                     # 存放项目相关的图片，如 HuggingKG 图表\n",
    "│   └── huggingkg.jpg         # HuggingKG 的图表文件\n",
    "├── HuggingKG/                # 用于构建 HuggingKG 知识图谱的代码和文档\n",
    "│   ├── HuggingKG_constructor.py  # 用于从 Hugging Face 爬取数据并构建知识图谱的主脚本\n",
    "│   ├── requirements.txt      # 运行构建脚本所需的 Python 包列表\n",
    "│   └── README.md             # 详细的构建过程和统计信息文档\n",
    "├── resource_recommendation/  # 资源推荐任务相关代码\n",
    "│   └── SSLRec/\n",
    "│       ├── config/           # 配置文件目录\n",
    "│       │   ├── configurator.py   # 解析命令行参数和 YAML 配置文件的脚本\n",
    "│       │   └── modelconf/    # 模型配置文件目录\n",
    "│       │       ├── bert4rec.yml    # BERT4Rec 模型的配置文件\n",
    "│       │       ├── dccf.yml        # DCCF 模型的配置文件\n",
    "│       │       ├── gformer.yml     # GFormer 模型的配置文件\n",
    "│       │       ├── hccf.yml        # HCCF 模型的配置文件\n",
    "│       │       ├── hmgcr.yml       # HMGCR 模型的配置文件\n",
    "│       │       ├── lightgcl.yml    # LightGCL 模型的配置文件\n",
    "│       │       ├── lightgcn.yml    # LightGCN 模型的配置文件\n",
    "│       │       └── directau.yml    # DirectAU 模型的配置文件\n",
    "│       ├── data_utils/       # 数据处理工具目录\n",
    "│       │   ├── datasets_diff.py          # 差异数据集处理类\n",
    "│       │   ├── datasets_general_cf.py    # 通用协同过滤数据集处理类\n",
    "│       │   ├── datasets_sequential.py    # 序列数据集处理类\n",
    "│       │   ├── datasets_multi_behavior.py# 多行为数据集处理类\n",
    "│       │   ├── data_handler_sequential.py# 序列数据处理类\n",
    "│       │   ├── data_handler_multi_behavior.py # 多行为数据处理类\n",
    "│       │   ├── data_handler_kg.py        # 知识图谱数据处理类\n",
    "│       │   ├── data_handler_social.py    # 社交数据处理类\n",
    "│       │   ├── data_handler_general_cf.py# 通用协同过滤数据处理类\n",
    "│       │   └── __init__.py       # 包初始化文件\n",
    "│       └── scripts/        # 实验脚本目录\n",
    "│           ├── run_general_cf.sh    # 运行通用协同过滤 collaborative filtering 实验的脚本 \n",
    "│           ├── run_social_rec.sh    # 运行社交推荐 social recommendation 实验的脚本\n",
    "│           ├── run_kg_based_rec.sh  # 运行基于知识图谱推荐 HuggingKG recommendation 实验的脚本\n",
    "├── task_classification/      # 任务分类任务相关代码\n",
    "│   └── tune_huggingface.py   # 任务分类实验的调优脚本\n",
    "└── model_tracing/            # 模型追踪任务相关代码\n",
    "    ├── kge/\n",
    "    │   ├── scripts/\n",
    "    │   │   ├── train.sh      # 模型训练脚本\n",
    "    │   │   └── test.sh       # 模型测试脚本\n",
    "    │   └── examples/\n",
    "    │       ├── huggingface-train-complex.yaml    # Complex 模型训练配置文件\n",
    "    │       ├── huggingface-train-rotate.yaml     # Rotate 模型训练配置文件\n",
    "    │       ├── huggingface-train-transe.yaml     # TransE 模型训练配置文件\n",
    "    │       └── huggingface-train-transformer.yaml# Transformer 模型训练配置文件\n",
    "```\n",
    "\n",
    "\n",
    "### 主要任务\n",
    "该项目围绕Hugging Face知识图谱开展了资源推荐、任务分类和模型追踪三个主要任务的基准测试，了解各个模型在不同指标下的性能表现，从而为模型的选择和优化提供参考依据。 \n",
    "\n",
    "### 具体操作及结果\n",
    "\n",
    "#### 1. HuggingKG知识图谱构建\n",
    "- **操作**：使用`HuggingKG_constructor.py`脚本从Hugging Face爬取数据并构建知识图谱。脚本中使用了多线程加速数据处理，通过`run`方法顺序执行数据收集、验证和存储操作。具体的数据来源和处理方式如下：\n",
    "    - **实体**：从不同的API端点和数据标签中获取任务、模型、数据集、空间、集合、论文、用户和组织等实体信息。\n",
    "    - **关系**：根据模型、数据集等数据中的标签信息，提取模型与任务、模型与模型、模型与数据集等之间的关系。\n",
    "- **结果**：构建了一个名为HuggingKG的综合知识图谱，包含丰富的节点和边信息。知识图谱的数据以JSON文件的形式存储，包括实体数据、关系数据和额外的ID集合数据。可在 [Hugging Face](https://huggingface.co/datasets/cqsss/HuggingKG) 上获取，其中`triples.txt`包含图三元组集合，`HuggingKG_V20241215174821.zip`包含详细节点和边属性的JSON文件。\n",
    "\n",
    "```plaintext\n",
    "# triples.txt\n",
    "\n",
    "# 节点 ->边-> 节点\n",
    "# JeffreyXiang/TRELLIS空间 使用了 JeffreyXiang/TRELLIS-image-large模型\n",
    "JeffreyXiang/TRELLIS space_use_model JeffreyXiang/TRELLIS-image-large\n",
    "\n",
    "# Changg/ori 模型 用于 “文本到图像” 任务\n",
    "Changg/ori model_definedFor_task text-to-image\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. 资源推荐\n",
    "- **操作**：使用 [SSLRec](https://github.com/HKUDS/SSLRec) 实现资源推荐任务。具体步骤如下：\n",
    "    1. 克隆`SSLRec`并安装依赖。\n",
    "    2. 将HuggingBench数据放置在`SSLRec/datasets`目录下。\n",
    "    3. 复制配置文件`./resource_recommendation/SSLRec/config`。\n",
    "    4. 运行`./resource_recommendation/SSLRec/scripts`中的脚本。\n",
    "- **结果**：在通用协同过滤、社交推荐和基于知识图谱的推荐任务上，评估了多种算法（如LightGCN、HCCF、SimGCL等）的性能，记录了不同算法在Recall和NDCG指标下的表现。\n",
    "\n",
    "测试指标\n",
    " - **Recall@k**：召回率，指在真实相关的项目中，被模型推荐到前 `k` 个位置的项目比例。例如，`Recall@5` 表示真实相关的项目中，有多少比例被模型推荐到了前 5 个位置。该指标值越高，说明模型能召回更多相关的项目。\n",
    " - **NDCG@k（Normalized Discounted Cumulative Gain）**：归一化折损累积增益，综合考虑了推荐结果的相关性和排名顺序，数值越高表示推荐结果越相关且排名越合理。\n",
    "\n",
    "以“General Collaborative Filtering”部分为例：\n",
    "|  | Recall@5 | Recall@10 | Recall@20 | Recall@40 | NDCG@5 | NDCG@10 | NDCG@20 | NDCG@40 |\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "| LightGCN | 0.0856 | 0.1301 | 0.1932 | 0.2759 | 0.0868 | 0.1003 | 0.1192 | 0.1413 |\n",
    "| HCCF | 0.0834 | 0.1254 | 0.1820 | 0.2504 | 0.0847 | 0.0975 | 0.1143 | 0.1328 |\n",
    "| SimGCL | 0.0999 | 0.1515 | 0.2186 | 0.3010 | 0.0998 | 0.1158 | 0.1358 | 0.1581 |\n",
    "| LightGCL | 0.1033 | 0.1558 | 0.2228 | 0.3017 | 0.1035 | 0.1198 | 0.1398 | 0.1611 |\n",
    "| AutoCF | 0.1003 | 0.1530 | 0.2190 | 0.3039 | 0.1012 | 0.1174 | 0.1371 | 0.1598 |\n",
    "| DCCF | 0.0985 | 0.1493 | 0.2167 | 0.3003 | 0.0983 | 0.1142 | 0.1343 | 0.1567 |\n",
    "\n",
    "从表格数据可以看出，`LightGCL` 模型在 `Recall@k` 和 `NDCG@k` 指标上的表现相对较好，说明该模型在召回相关项目和提供合理推荐排名方面具有优势。而 `HCCF` 模型的各项指标值相对较低，性能相对较弱。\n",
    "\n",
    "#### 3. 任务分类\n",
    "- **操作**：使用 [CogDL](https://github.com/THUDM/CogDL) 实现任务分类任务。具体步骤如下：\n",
    "    1. 克隆`CogDL`并安装依赖。\n",
    "    2. 下载 [HuggingBench-Classification](https://huggingface.co/datasets/cqsss/HuggingBench-Classification) 数据到`task_classification/data/`目录。\n",
    "    3. 运行`./task_classification/tune_huggingface.py`。\n",
    "- **结果**：评估了多种图神经网络模型（如GCN、GAT、GRAND等）在任务分类任务上的性能，记录了不同模型在不同特征表示（如binary、BERT、BGE等）下的表现。\n",
    "\n",
    "\n",
    "每一行代表一个不同的图神经网络模型，这些模型被用于任务分类任务，具体解释如下：\n",
    "- **GCN（Graph Convolutional Network）**：图卷积网络，是一种经典的图神经网络，通过聚合节点的邻域信息来学习节点的表示。\n",
    "- **GAT（Graph Attention Network）**：图注意力网络，引入了注意力机制，能够自适应地为不同邻域节点分配不同的权重。\n",
    "- **GRAND（Graph Random Neural Network）**：一种基于随机游走的图神经网络，通过随机采样和聚合邻域信息来学习节点表示。\n",
    "- **GraphSAGE（Graph Sample and Aggregate）**：图采样与聚合网络，通过采样和聚合邻域节点的信息来学习节点表示，适用于大规模图数据。\n",
    "- **ANNPN（Adaptive Neural Network Propagation Network）**：自适应神经网络传播网络，可能是一种自定义的图神经网络模型，用于处理特定的图数据和任务。\n",
    "- **GCNII（Graph Convolutional Network II）**：可能是 GCN 的改进版本，通过引入额外的层和机制来提高模型的性能。\n",
    "- **GraphSAINT（Graph Sampling Based Inductive Learning Method）**：基于图采样的归纳学习方法，通过采样子图来训练模型，适用于大规模图数据。\n",
    "- **RevGCN（Reversible Graph Convolutional Network）**：可逆图卷积网络，引入了可逆模块，能够减少内存消耗和提高训练效率。\n",
    "- **RevGAT（Reversible Graph Attention Network）**：可逆图注意力网络，结合了可逆模块和注意力机制，用于处理图数据。\n",
    "\n",
    "每一列代表一种不同的特征表示方法，用于为模型提供输入数据，具体解释如下：\n",
    "- **binary**：二进制特征表示，可能是将数据转换为二进制向量作为输入。\n",
    "- **BERT**：使用预训练的 BERT 模型提取的特征，BERT 是一种基于 Transformer 的预训练语言模型，能够学习到丰富的语义信息。\n",
    "- **BERT (ft)**：在预训练的 BERT 模型基础上进行微调（fine-tuning）后提取的特征，微调可以使模型更好地适应特定的任务。\n",
    "- **BGE**：可能是一种自定义的特征提取方法，用于提取图数据的特征。“Bidirectional Graph Embedding” 双向图嵌入\n",
    "- **BGE (ft)**：在 BGE 特征提取方法的基础上进行微调后提取的特征。 “fine-tuned”（微调）\n",
    "\n",
    "|            | binary  | BERT    | BERT (ft) | BGE     | BGE (ft)  |\n",
    "|------------|---------|---------|---------|---------|---------|\n",
    "| GCN        | 0.0662  | 0.7620  | 0.8291  | 0.7411  | 0.8522  |\n",
    "| GAT        | 0.0390  | 0.5105  | 0.8125  | 0.5444  | 0.8261  |\n",
    "| GRAND      | 0.1228  | 0.1297  | 0.6089  | 0.2646  | 0.4532  |\n",
    "| GraphSAGE  | 0.1800  | 0.5341  | 0.8845  | 0.8199  | 0.8830  |\n",
    "| ANNPN      | 0.0448  | 0.7297  | 0.8304  | 0.7571  | 0.8419  |\n",
    "| GCNII      | 0.1149  | 0.6456  | 0.8836  | 0.7779  | 0.8802  |\n",
    "| GraphSAINT | 0.0579  | 0.2703  | 0.8342  | 0.0540  | 0.8251  |\n",
    "| RevGCN     | 0.1071  | 0.6763  | 0.8851  | 0.8039  | 0.8770  |\n",
    "| RevGAT     | 0.0335  | 0.7412  | 0.8849  | 0.7569  | 0.8716  |\n",
    "\n",
    "从表格数据可知，`GraphSAGE` 模型在 `BERT (ft)` 和 `BGE` 特征表示下的分类准确率较高，说明该模型在这两种特征下的分类性能较好。而 `GRAND` 模型在部分特征表示下的准确率较低，性能相对较差。\n",
    "\n",
    "\n",
    "\n",
    "#### 4. 模型溯源\n",
    "- **操作**：使用 [LibKGE](https://github.com/uma-pi1/kge) 知识图嵌入（Knowledge Graph Embedding，KGE）实现有监督基线；使用 [ULTRA](https://github.com/DeepGraphLearning/ULTRA) 和 [KG-ICL](https://github.com/nju-websoft/KG-ICL) 的官方代码实现两个无监督模型。具体步骤如下：\n",
    "    1. 克隆`LibKGE`并安装依赖。\n",
    "    2. 下载 [HuggingBench-Tracing](https://huggingface.co/datasets/cqsss/HuggingBench-Tracing) 数据到`kge/data/huggingface`。\n",
    "    3. 复制配置文件`./model_tracing/kge/examples`。\n",
    "    4. 运行训练/测试脚本`model_tracing\\kge\\scripts\\train.sh`和`model_tracing\\kge\\scripts\\test.sh`。\n",
    "- **结果**：评估了多种模型（如RESCAL、TransE、DistMult等）在模型追踪任务上的性能，记录了不同模型在MRR和HIT@k指标下的表现。\n",
    "\n",
    "\n",
    "测试指标\n",
    " - **MRR（Mean Reciprocal Rank）**：平均倒数排名，反映了模型预测结果中正确答案的排名情况，数值越高表明模型预测的结果越靠前，性能越好。\n",
    " - **HIT@k**：指在模型预测的前 `k` 个结果中，包含正确答案的比例。例如，`HIT@1` 表示第一个预测结果就是正确答案的比例，`HIT@3` 表示前三个预测结果中包含正确答案的比例，以此类推。该指标值越高，说明模型的命中率越高。\n",
    "\n",
    "\n",
    "|  | MRR | HIT@1 | HIT@3 | HIT@5 | HIT@10 |\n",
    "|--|--|--|--|--|--|\n",
    "| RESCAL | 0.2694 | 0.2380 | 0.2667 | 0.2929 | 0.3470 |\n",
    "| TransE | 0.5589 | 0.4496 | 0.6321 | 0.6973 | 0.7562 |\n",
    "| DistMult | 0.2050 | 0.1421 | 0.2321 | 0.2735 | 0.3324 |\n",
    "| ComplEx | 0.1807 | 0.1109 | 0.2122 | 0.2599 | 0.3066 |\n",
    "| ConvE | 0.4739 | 0.3766 | 0.5119 | 0.5903 | 0.6735 |\n",
    "| RotatE | 0.5317 | 0.4195 | 0.6029 | 0.6803 | 0.7392 |\n",
    "| HittER | 0.3678 | 0.2900 | 0.4078 | 0.4657 | 0.5314 |\n",
    "| ULTRA(无监督模型) | 0.3373 | 0.1440 | 0.4803 | 0.5309 | 0.6672 |\n",
    "| KG - ICL(无监督模型) | 0.4008 | 0.3354 | 0.3792 | 0.4854 | 0.5938 |\n",
    "\n",
    "从表格数据可知，`TransE` 模型在各项指标上的表现相对较好，尤其是 `MRR` 和 `HIT@k` 指标值都比较高，这表明 `TransE` 模型在预测时能将正确答案排在较前的位置，且命中率较高。而 `ComplEx` 模型的各项指标值相对较低，说明其性能在这些模型中相对较差。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e164d1-aedf-474a-8db2-87925925f660",
   "metadata": {},
   "source": [
    "# 图神经网络\n",
    "\n",
    "图神经网络（GNN）的精髓确实在于“邻居聚合+更新”这个循环过程。\n",
    "\n",
    " 一、 图神经网络的核心目标是什么？ \n",
    "\n",
    "GNN 的目标是 学习图结构数据的有效表示（嵌入） 。具体来说，它可以学习：\n",
    "1. 节点表示：  每个节点的向量，编码了其自身特征、邻居特征以及在整个图中的结构位置信息（例如：用户画像、蛋白质功能）。\n",
    "2. 边表示：  每条边的向量，编码了连接关系及其特征（例如：社交关系强度、分子键类型）。\n",
    "3. 图表示：  整个图的向量，编码了图的全局结构和所有节点/边的信息（例如：分子属性、社交网络社区性质）。\n",
    "\n",
    " 这些学习到的表示（嵌入）可以直接用于下游任务： \n",
    "*  节点级任务：  节点分类（如用户类型识别、蛋白质功能预测）、节点回归（如用户价值预测）。\n",
    "*  边级任务：  链接预测（如推荐好友、预测药物相互作用）、边分类（如关系类型识别）。\n",
    "*  图级任务：  图分类（如分子性质预测、社交网络社区类型识别）、图回归（如预测材料的整体性能）。\n",
    "\n",
    "流程：\n",
    "1. 输入原始图数据： `(节点特征 X, 邻接矩阵 A, [边特征 E])`\n",
    "2. 通过GNN编码器： `GNN(X, A, [E]) -> 嵌入` (例如 `节点嵌入 H`, `图嵌入 h_G`)\n",
    "3. （可选）图池化 (针对图任务)： `POOL(H) -> h_G`\n",
    "4. 输入下游模型： `下游模型(嵌入) -> 预测结果`\n",
    "    * *节点任务：* `分类器/回归器(h_v) -> 节点标签/值`\n",
    "    * *边任务：* `解码器(h_u, h_v) -> 边存在概率/边标签`\n",
    "    * *图任务：* `分类器/回归器(h_G) -> 图标签/值`\n",
    "\n",
    "\n",
    " 二、 图数据带来的核心挑战是什么？ \n",
    "\n",
    "1. 非欧几里得结构：  图不像图像（规则网格）或文本（序列）那样具有固定、规则的结构。节点数量可变，邻居数量可变，连接方式任意（拓扑结构复杂）。\n",
    "2. 依赖关系：  节点的属性或标签往往 高度依赖其邻居节点 （同质性）和 在整个图中所处的位置 （结构角色）。例如：社交网络中，朋友的朋友可能也是朋友；分子中，一个原子的性质受其化学键连接的原子影响。\n",
    "3. 置换不变性：  图的信息与其节点编号顺序无关。聚合邻居信息时，无论邻居的排列顺序如何，结果应该相同。\n",
    "\n",
    " 三、 GNN 的解决之道：邻居聚合 + 消息传递 + 分层传播 \n",
    "\n",
    "GNN 的核心思想是： 一个节点的表示，应该由其自身的特征和其邻居节点的表示共同决定。  这个过程是 迭代 进行的。\n",
    "\n",
    " 核心步骤分解（“邻居聚合+更新”详解）： \n",
    "\n",
    "想象你在一个社交网络中，你想了解“张三”这个人（目标节点）。\n",
    "\n",
    "1. 初始化： \n",
    "  * 第 0 层 (`l=0`)：每个节点 `v` 的初始表示 `h_v⁽⁰⁾` 就是它自身的输入特征向量 `x_v`。例如：张三的初始表示 = [年龄, 职业, 兴趣...]。\n",
    "\n",
    "2. 迭代/分层处理 (对于每一层 `l = 0, 1, ..., L-1`)： \n",
    "  *  a. 邻居聚合： (收集信息) \n",
    "  *  目标：  对于目标节点 `v`，收集其 所有邻居节点 `u ∈ N(v)`  在 当前层 `l`  的表示 `h_u⁽ˡ⁾`。\n",
    "  *  关键：  如何把多个邻居的信息合并成一个 聚合向量  `a_v⁽ˡ⁺¹⁾`？\n",
    "  *  常用聚合函数 (需满足置换不变性)： \n",
    "  *  求和：  `a_v⁽ˡ⁺¹⁾ = SUM({ h_u⁽ˡ⁾ for u in N(v) })` - 强调邻居信息的强度。\n",
    "  *  求平均：  `a_v⁽ˡ⁺¹⁾ = MEAN({ h_u⁽ˡ⁾ for u in N(v) })` - 获取邻居信息的平均水平。\n",
    "  *  取最大值：  `a_v⁽ˡ⁺¹⁾ = MAX({ h_u⁽ˡ⁾ for u in N(v) })` - 关注邻居中最突出的特征。\n",
    "  *  注意力加权求和 (如GAT)：  `a_v⁽ˡ⁺¹⁾ = SUM( α_{vu} * h_u⁽ˡ⁾ for u in N(v) )` - 其中 `α_{vu}` 是学习得到的注意力权重，表示邻居 `u` 对目标 `v` 的重要性。这是最灵活强大的方式！\n",
    "  *  类比：  张三想了解自己，他先问问他的朋友们（邻居）：“你们最近咋样？” 然后把朋友们的回答（`h_u⁽ˡ⁾`）汇总整理成一个综合印象（`a_v⁽ˡ⁺¹⁾`）。汇总方式可以是简单记录所有人说了啥（求和），计算平均观点（平均），只记住最极端的观点（最大值），或者根据他对每个朋友的信任度给不同朋友的发言加权（注意力）。\n",
    "  *  b. 节点更新： (融合信息 + 生成新表示) \n",
    "  *  目标：  目标节点 `v`  结合 它自己在 上一层的表示 `h_v⁽ˡ⁾`  和  刚刚聚合得到的邻居信息 `a_v⁽ˡ⁺¹⁾` ，生成它在 下一层 `l+1`  的 新表示  `h_v⁽ˡ⁺¹⁾`。\n",
    "  *  如何结合？  使用一个 可学习的更新函数 （通常是一个简单的神经网络）：\n",
    "  *  常见形式： \n",
    "  *  拼接 + 线性变换 + 激活：  `h_v⁽ˡ⁺¹⁾ = σ( W⁽ˡ⁾ * CONCAT(h_v⁽ˡ⁾, a_v⁽ˡ⁺¹⁾) )` - `W⁽ˡ⁾` 是层特定的可学习权重矩阵，`σ` 是非线性激活函数（如ReLU）。\n",
    "  *  相加 + 线性变换 + 激活 (如经典GCN)：  `h_v⁽ˡ⁺¹⁾ = σ( W⁽ˡ⁾ * (h_v⁽ˡ⁾ + a_v⁽ˡ⁺¹⁾) )` - 更强调信息的融合。\n",
    "  *  类比：  张三听完朋友们的综合意见 (`a_v⁽ˡ⁺¹⁾`) 后，结合自己之前的想法 (`h_v⁽ˡ⁾`)，经过一番思考（`W⁽ˡ⁾` 和 `σ`），形成了对自己更新的认识 (`h_v⁽ˡ⁺¹⁾`)。这个新认识融合了他自己的观点和朋友们的观点。\n",
    "\n",
    "3. 分层传播与感受野： \n",
    "  * 上述聚合和更新过程在每一层 `l` 都会发生一次。\n",
    "  *  第 1 层 (`l=1`)：  `h_v⁽¹⁾` 包含了 `v` 自身特征 (`h_v⁽⁰⁾`) 和其 一阶邻居（直接朋友）  的特征 (`a_v⁽¹⁾` 来自 `h_u⁽⁰⁾`)。\n",
    "  *  第 2 层 (`l=2`)：  `h_v⁽²⁾` 的计算依赖于：\n",
    "  * 邻居 `u` 在 `l=1` 的表示 `h_u⁽¹⁾` (这些 `h_u⁽¹⁾` 本身已经包含了 `u` 自身和 `u` 的一阶邻居的信息)。\n",
    "  * 因此，`h_v⁽²⁾` 实际上包含了 `v` 自身、`v` 的一阶邻居 (`N(v)`)、以及 `v` 的 二阶邻居（朋友的朋友）  的信息！\n",
    "  *  第 K 层 (`l=K`)：  `h_v⁽ᴷ⁾` 包含了 `v` 的  K-hop 邻居范围 内的信息。\n",
    "  *  最终表示：  通常，最后一层 `L` 的输出 `h_v⁽ᴸ⁾` 就作为节点 `v` 的最终表示（嵌入），用于下游任务。\n",
    "\n",
    " 四、 关键特性与优势 \n",
    "\n",
    "1. 局部性：  每一层的计算只涉及局部邻居，计算高效。\n",
    "2. 置换不变性：  聚合函数保证了邻居顺序不影响结果。\n",
    "3. 参数共享：  同一层的所有节点使用相同的聚合函数和更新函数（相同的 `W⁽ˡ⁾` 等参数）。这使得模型参数量与图大小无关，能处理任意大小的图，并具有良好的泛化能力。\n",
    "4. 感受野可控：  通过堆叠层数 (`L`)，可以控制节点获取信息的范围（从局部到全局）。层数越多，“视野”越广。\n",
    "\n",
    " 五、 一个简化实例 (以GCN层为例) \n",
    "\n",
    "假设一个简单无向图：\n",
    "* 节点：A (连接B, C), B (连接A), C (连接A)\n",
    "* 初始特征：`h_A⁽⁰⁾ = [1, 0]`, `h_B⁽⁰⁾ = [0, 1]`, `h_C⁽⁰⁾ = [1, 1]`\n",
    "* 邻接矩阵 `A` (带自环 `Ã = A + I`)，度矩阵 `D̃` (对角线为节点度数+1)\n",
    "* 简化传播规则：`H⁽ˡ⁺¹⁾ = σ( D̃⁻¹ᐟ² Ã D̃⁻¹ᐟ² H⁽ˡ⁾ W⁽ˡ⁾ )` (忽略激活函数 `σ` 和权重 `W⁽ˡ⁾` 的影响，只看聚合效果)\n",
    "\n",
    " 计算节点 A 在 `l=1` 的表示： \n",
    "\n",
    "1. 归一化：  `D̃⁻¹ᐟ² Ã D̃⁻¹ᐟ²` 的作用是对邻居特征进行 归一化的平均聚合 。\n",
    "  * A 的度 `d̃_A = 3` (连接B, C 和自己)，所以 `1/sqrt(d̃_A) ≈ 0.577`\n",
    "  * B 的度 `d̃_B = 2`, `1/sqrt(d̃_B) ≈ 0.707`\n",
    "  * C 的度 `d̃_C = 2`, `1/sqrt(d̃_C) ≈ 0.707`\n",
    "  * 对于 A，聚合其邻居 B, C, A (自身)：\n",
    "  * B 的贡献权重：`(1/sqrt(d̃_A)) * (1/sqrt(d̃_B)) ≈ 0.577 * 0.707 ≈ 0.408`\n",
    "  * C 的贡献权重：`(1/sqrt(d̃_A)) * (1/sqrt(d̃_C)) ≈ 0.577 * 0.707 ≈ 0.408`\n",
    "  * A (自身) 的贡献权重：`(1/sqrt(d̃_A)) * (1/sqrt(d̃_A)) ≈ 0.577 * 0.577 ≈ 0.333`\n",
    "2. 聚合 (实质是加权平均)： \n",
    "  * `a_A⁽¹⁾ = (0.333 * [1, 0]) + (0.408 * [0, 1]) + (0.408 * [1, 1])`\n",
    "  * 计算：\n",
    "  * 第一维：`0.333*1 + 0.408*0 + 0.408*1 = 0.333 + 0 + 0.408 = 0.741`\n",
    "  * 第二维：`0.333*0 + 0.408*1 + 0.408*1 = 0 + 0.408 + 0.408 = 0.816`\n",
    "  * 所以 `a_A⁽¹⁾ ≈ [0.741, 0.816]`\n",
    "3. 更新 (在GCN中，聚合结果 `a_A⁽¹⁾` 就是 `h_A⁽¹⁾`，因为更新函数是恒等或线性)： \n",
    "  * `h_A⁽¹⁾ ≈ [0.741, 0.816]`\n",
    "\n",
    " 结果解释： \n",
    "* A 的新特征 `[0.741, 0.816]` 不再是原始的 `[1, 0]`。\n",
    "* 它融合了：\n",
    "  * 自身的原始特征 (`[1, 0]`，权重 0.333)\n",
    "  * 邻居 B 的特征 (`[0, 1]`，权重 0.408)\n",
    "  * 邻居 C 的特征 (`[1, 1]`，权重 0.408)\n",
    "* 数值上更接近 C (`[1, 1]`)，因为 B (`[0, 1]`) 和 C (`[1, 1]`) 在第二维都有值，且权重相似，而 A 自身在第二维是 0。这反映了邻居信息对 A 表示的影响。\n",
    "\n",
    " 六、 总结 \n",
    "\n",
    "1. 问题：  如何利用复杂、不规则的图结构（节点+边+关系）来学习有效的节点/边/图表示？\n",
    "2. 目标：  学习编码了 自身特征 、 局部邻域结构 和 全局位置信息 的表示（嵌入）。\n",
    "3. 核心解决步骤 (消息传递框架)：  分层迭代执行\n",
    "  *  消息传递 (可选但常用)：  邻居节点 `u` 基于其当前状态 `h_u⁽ˡ⁾` 生成要发送给目标节点 `v` 的消息 `m_u⁽ˡ⁾` (简单情况 `m_u⁽ˡ⁾ = h_u⁽ˡ⁾`)。\n",
    "  *  邻居聚合：  目标节点 `v` 使用 置换不变函数  (Sum, Mean, Max, Attention) 聚合从所有邻居 `u ∈ N(v)` 收到的消息 `m_u⁽ˡ⁾`，得到聚合向量 `a_v⁽ˡ⁺¹⁾`。\n",
    "  *  节点更新：  目标节点 `v` 使用一个 可学习的函数  (通常是神经网络) 将 自身上一层的表示 `h_v⁽ˡ⁾`  和 聚合得到的邻居信息 `a_v⁽ˡ⁺¹⁾`  融合起来，生成自己 在下一层的新表示 `h_v⁽ˡ⁺¹⁾` 。\n",
    "4. 信息传播：  通过堆叠 `L` 层，每个节点最终的表征 `h_v⁽ᴸ⁾` 能够捕获其 `L-hop` 邻域内的信息。层数 (`L`) 决定了节点“看到”的图范围大小。\n",
    "\n",
    " “邻居聚合+更新”的本质：  就是让图中的每个节点，通过不断地“倾听”并“融合”周围邻居的声音（信息），同时结合自己之前的想法，逐步更新完善对自身（以及最终对整个图）的认识。这个过程在图的连接结构引导下，层层递进，最终让每个节点都拥有了蕴含丰富局部和全局信息的“智慧”。 \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
