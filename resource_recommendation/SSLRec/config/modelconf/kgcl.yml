optimizer:
  name: adam
  lr: 1.0e-3 # not 1e-3
  weight_decay: 0

train:
  trainer: kgcl_trainer
  epoch: 100
  batch_size: 1024
  kg_batch_size: 1024
  save_model: true
  loss: pairwise # bpr
  test_step: 3 # evaluate per {test_step} epochs
  patience: 5 # early stop patience
  #pretrain_path: ./checkpoint/xxxx.pth
  reproducible: true
  seed: 42

test:
  metrics: [mrr, recall, ndcg]
  k: [5, 10, 20, 40]
  batch_size: 4096
  eval_at_one_forward: true

data:
  type: kg # choose in {general_cf, multi_behavior, sequential, social, kg}
  name: huggingface

model:
  name: kgcl # case-insensitive
  train_trans: false
  layer_num: 2
  layer_num_kg: 2
  decay_weight: 1.0e-5
  embedding_size: 64
  node_dropout: true
  node_dropout_rate: 0.5
  mess_dropout: true
  mess_dropout_rate: 0.1

  tau: 0.2
  cl_weight: 0.1
  mu: 0.7

tune:
  enable: true
  hyperparameters: [lr, layer_num]
  lr: [1.0e-3]
  layer_num: [2, 3]
